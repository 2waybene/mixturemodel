# read in the raw D.I. value
##============================
dt <- read.csv (f_IN)
## determine how many families are we dealing with
numOfFamily <-  1 # minimun one family
if (length(which(as.vector(dt$DNA_Index) > aneuThresh)) > 1)
{
numOfFamily = 3
}else if (length(which(as.vector(dt$DNA_Index) > mitoThresh)) > 1)
{
numOfFamily = 2
}
numOfFamily
##===================================================
##  removing the normal family
##  upto two round
##===================================================
dt.raw  <- ""
firstDT <- ""
get.gen <- ""
peaks   <- c()
dt.raw <- as.vector (dt$DNA_Index)
tol.num.of.dt <- length(dt.raw)
get.den <- density(dt.raw)
peaks <- peak.quick (get.den$x, get.den$y)
peaks
##===================================================
##  Determine where to start the first population
##  There could be more small peaks less than 1
##  Try to get the first one peaks > 1 but < 1.2
##====================================================
index = 1
length(which(peaks < 1))
if (peaks[length(which(peaks<1)) + 1] < 1.2)
{
index = length(which(peaks<1)) + 1
}else { index = length(which(peaks<1)) }
index
##============================================
##  clean starts here with first population
##============================================
firstDT <- getPopWIndex (dt.raw, index)
##  Save first population dt
FP_dt_primary <- firstDT + peaks[index]
dt.cleaned <- cleanFirstPop(peaks[index], firstDT, dt.raw)
#temp <- followUpClean (peaks[index], firstDT, dt.raw)
#plot(density(dt.cleaned))
#str(dt.cleaned)
##================================
##  Second round if ever needed
##================================
dt.raw  <- dt.cleaned
firstDT <- ""
get.gen <- ""
peaks   <- c()
index = 1
firstDT <- getPopWIndex (dt.raw, index)
#plot(density(firstDT))
get.den <- density(dt.raw)
peaks <- peak.quick (get.den$x, get.den$y)
peaks
##=========================================
##  Follow the same protocol, but just
##  carry out one more cleaning cycle
##  if there is any peaks less than 1.2
##===========================================
##Need to add the "cleaned back to population one"!!
dt.clean.return = list()
if (peaks[1] < 1.2)
{
#dt.another.clean <- cleanFirstPop(peaks[1], firstDT, dt.cleaned)
dt.clean.return <- followUpClean (peaks[1], firstDT, dt.raw)
#  plot(density(dt.another.clean))
#  dt.1pop.cleaned <- dt.another.clean
#if (length(dt.clean.return$dtFiltered) > 0) #
if (length(dt.clean.return$dtFiltered) > 1) #FIXME, there was a bug
{
FP_dt_primary <- c(FP_dt_primary, dt.clean.return$dtFiltered)
}
dt.1pop.cleaned <- dt.clean.return$dtRetain
}else{
dt.1pop.cleaned <- dt.cleaned
}
FP_mean <- mean(FP_dt_primary)
FP_std <- sd(FP_dt_primary)
FP_count <- tol.num.of.dt  - length(dt.1pop.cleaned)
FP <- list ("FP_mean" = FP_mean, "FP_std" = FP_std, "FP_count" = FP_count)
cleanedSample <- c(cleanedSample, FP)
cleanedSample
FP_dt_primary
num.of.DI.left <- length(dt.1pop.cleaned)
get.den <- density(dt.1pop.cleaned)
peaks <- peak.quick (get.den$x, get.den$y)
peaks
num.of.DI.left
plot(dt.1pop.cleaned)
plot(density(dt.1pop.cleaned))
index = 1
if (length(which(peaks < 1.5)) >=1 )
{
index <-which(peaks > 1.5)[1]
}
index
secondDT <- getPopWIndex (dt.1pop.cleaned, index)
dt.1pop.cleaned
index)
index
str(dt.1pop.cleaned)
str(secondDT)
SP_dt_primary <- (secondDT + peaks[index])
secondDT.cleaned <- cleanFirstPop(peaks[index],  secondDT, dt.1pop.cleaned)
#str(secondDT.cleaned)
secondDT.cleaned
peaks[index]
secondDT
dt.1pop.cleaned
stats(secondDT)
##==================================
get.den <- density(secondDT.cleaned)
peaks <- peak.quick (get.den$x, get.den$y)
peaks
##  Determine where to start the first population
index = 0
third_round = 0
if (length(peaks) > 1 &  length(which(peaks < 2)) >= 1)
{
index =  which(peaks < 2) [length(which(peaks < 2))]
}
third_round
index
if (index >=1)
{
secondDT.1 <- getPopWIndex (secondDT.cleaned, index)
#plot(density(secondDT.1))
#plot(density(secondDT.1 + peaks[index]))
secondDT.2.cleaned <- cleanFirstPop(peaks[index],  secondDT.1, secondDT.cleaned)
third_round = 1
#str(secondDT.2.cleaned)
#plot(density(secondDT.2.cleaned))
#stats (secondDT.2.cleaned)
}else{
secondDT.2.cleaned <- secondDT.cleaned
}
secondDT.2.cleaned
library(ks)
set.seed(1)
par(mfrow=c(2,1))
x<-rlnorm(100)
hist(x, col="red", freq=F)
lines(density(x))
y <- rkde(fhat=kde(x=x, h=hpi(x)), n=100, positive=TRUE)
hist(y, col="green", freq=F)
#	simulating DNA D.I. values
# Need to source ~/myGit/mixturemodel/Scripts/simDt_functions.R
#	normal population
library(Rlab)
##  OS specific directories:
mac.os  <- "/Users/li11/"
linux   <- "~/"
windows <- "X:/"
root <- mac.os
source (paste (root, "/myGit/mixturemodel/Scripts/simDt_functions.R", sep=""))
source (paste (root, "/myGit/mixturemodel/Scripts/simDt_functions.R", sep=""))
mean.norm <- c()
for (i in 1:20)
{
mean.norm[i] <- runif (1, 0.9,1.2)
}
mean.normalP <- mean.norm[sample(1:20,1)]
#	mitotic population
mean.mitotic <- c()
for (i in 1:20)
{
mean.mitotic[i] <- runif (1, 1.7,2.2)
}
mean.mitoticP <- mean.mitotic[sample(1:20,1)]
#	aneuploidy  population
mean.aneu <- 3.3
st.aneu <-3.5
n = 40
sample.aneu <- rnorm(n, mean = mean.aneu, sd = st.aneu)
sample.aneuP <- sample.aneu[sample.aneu >=0]
plot(density(sample.aneuP))
mean.aneuP <- mean(sample.aneuP)
sigmaA <- sd(sample.aneuP)
#  simulating DNA D.I. values
# Need to source ~/myGit/mixturemodel/Scripts/simDt_functions.R
#	normal population
library(Rlab)
##  OS specific directories:
mac.os  <- "/Users/li11/"
linux   <- "~/"
windows <- "X:/"
##=============================================
##  Read in data
##=============================================
#root <- windows
root <- mac.os
dt.dir <- paste (root, "/myGit/mixturemodel/cleanedData/OSCC", sep="")
files <- list.files (pattern=".rda")
files
files <- list.files (path = dt.dir, pattern=".rda")
files
i=1
load(paste(dt.dir, files[i], sep=""))
files[i]
dt.dir <- paste (root, "/myGit/mixturemodel/cleanedData/OSCC/", sep="")
files <- list.files (path = dt.dir, pattern=".rda")
files
aneuMax = 0;
aneuMin = 0;
i=1
load(paste(dt.dir, files[i], sep=""))
str()
ls()
str(cleanedSample)
str(cleanedSample$Aneuleft)
str(cleanedSample$AneuLeft)
max(cleanedSample$AneuLeft)
aneuMax = 0;
aneuMin = 2.3;
for (i in 1:length(files))
{
load(paste(dt.dir, files[i], sep=""))
if (aneuMax < max(cleanedSample$AneuLeft))
{
aneuMax = max(cleanedSample$AneuLeft))
}
}
i=1
load(paste(dt.dir, files[i], sep=""))
for (i in 1:length(files))
{
load(paste(dt.dir, files[i], sep=""))
if (aneuMax < max(cleanedSample$AneuLeft))
{
aneuMax = max(cleanedSample$AneuLeft)
}
}
for (i in 1:length(files))
{
load(paste(dt.dir, files[i], sep=""))
if (cleanedSample$AneuLeft) != "")
{
if (aneuMax < max(cleanedSample$AneuLeft))
{
aneuMax = max(cleanedSample$AneuLeft)
}
}
}
for (i in 1:length(files))
{
load(paste(dt.dir, files[i], sep=""))
if (cleanedSample$AneuLeft != "")
{
if (aneuMax < max(cleanedSample$AneuLeft))
{
aneuMax = max(cleanedSample$AneuLeft)
}
}
}
i
(cleanedSample$AneuLeft != "")
cleanedSample$AneuLeft
cleanedSample
cleanedSample$AneuLeft
length(cleanedSample$AneuLeft)
(length(cleanedSample$AneuLeft) != 0)
for (i in 1:length(files))
{
load(paste(dt.dir, files[i], sep=""))
if (length(cleanedSample$AneuLeft) != 0)
{
if (aneuMax < max(cleanedSample$AneuLeft))
{
aneuMax = max(cleanedSample$AneuLeft)
}
}
}
aneuMax
##  File: predModHong.R
##  Author: Hong Xu
library(caret)
library(pROC)
library(Metrics)
#====================
mac.os  <- "/Users/li11/"
linux   <- "~/"
windows <- "X:/"
#root <- windows
root <- mac.os
##### REF: http://stats.stackexchange.com/questions/31579/what-is-the-optimal-k-for-the-k-nearest-neighbour-classifier-on-the-iris-dat
# https://gist.github.com/zachmayer/3061272
#Multi-Class Summary Function
#Based on caret:::twoClassSummary
require(compiler)
multiClassSummary <- cmpfun(function (data, lev = NULL, model = NULL)
{
#Load Libraries
require(Metrics)
require(caret)
#Check data
if (!all(levels(data[, "pred"]) == levels(data[, "obs"])))
stop("levels of observed and predicted data do not match")
#Calculate custom one-vs-all stats for each class
prob_stats <- lapply(levels(data[, "pred"]), function(class)
{
#Grab one-vs-all data for the class
pred <- ifelse(data[, "pred"] == class, 1, 0)
obs <- ifelse(data[, "obs"] == class, 1, 0)
prob <- data[,class]
#Calculate one-vs-all AUC and logLoss and return
cap_prob <- pmin(pmax(prob, .000001), .999999)
prob_stats <- c(auc(obs, prob), logLoss(obs, cap_prob))
names(prob_stats) <- c("ROC", "logLoss")
return(prob_stats)
})
prob_stats <- do.call(rbind, prob_stats)
rownames(prob_stats) <- paste( "Class:" , levels(data[, "pred"]))
#Calculate confusion matrix-based statistics
CM <- confusionMatrix(data[, "pred"], data[, "obs"])
#Aggregate and average class-wise stats
#Todo: add weights
class_stats <- cbind(CM$byClass, prob_stats)
class_stats <- colMeans(class_stats)
#Aggregate overall stats
overall_stats <- c(CM$overall)
#Combine overall with class-wise stats and remove some stats we don't want
stats <- c(overall_stats, class_stats)
stats <- stats[! names(stats) %in% c("AccuracyNull","Prevalence", "Detection Prevalence")]
#Clean names and return
names(stats) <- gsub('[[:blank:]] +', '_' , names(stats))
return(stats)
})
## Note: no visible binding for global variable 'Metrics'
## Note: no visible binding for global variable 'caret'
## set up working directory
setwd(paste (root, "/myGit/mixturemodel/reconData/para1/", sep=""))
## read in data from txt file
data <- read.table("recon_3classes_para1.txt", header=TRUE, sep = "\t")
setwd(paste (root, "/myGit/mixturemodel/reconData/para2/", sep=""))
## read in data from txt file
data <- read.table("recon_3classes_para2.txt", header=TRUE, sep = "\t")
data <- read.table("recon_3classes_para3.txt", header=TRUE, sep = "\t")
data <- read.table("recon_3classes_para4.txt", header=TRUE, sep = "\t")
##### BEGIN: data partition >>>>>
## set random seed
set.seed(12345)
#set.seed(34546)
## create data partition
inTrainingSet <- createDataPartition(data$label, p=.7, list=FALSE)
labelTrain <- data[ inTrainingSet,]
labelTest <- data[-inTrainingSet,]
nrow(labelTrain)
nrow(labelTest)
##### END: data partition <<<<<
##### BEGIN: tune the parameters >>>>>
## control:
# resampling technique: 5-repeat 10-fold cross-validation
# performance metrics: ROC AUC curve
ctrl <- trainControl(method = "repeatedcv",
repeats = 5,
summaryFunction = multiClassSummary,
classProbs = TRUE)
##### END: tune the parameters <<<<<
##### BEGIN: train model - svm >>>>>
set.seed(1024)
svmFit <- train(label ~ ., data = labelTrain,
## training model: svm >>>
method = "svmRadial",
metric = "ROC",
tuneLength = 10,
trControl = ctrl)
## prediction
svmPred <- predict(svmFit, labelTest)
str(svmPred)
## predicted probabilities
svmProbs <- predict(svmFit, labelTest, type = "prob")
str(svmProbs)
confusionMatrix(svmPred, labelTest$label)
##  File: predModHong.R
##  Author: Hong Xu
library(caret)
library(pROC)
library(Metrics)
#====================
mac.os  <- "/Users/li11/"
linux   <- "~/"
windows <- "X:/"
#root <- windows
root <- mac.os
##### REF: http://stats.stackexchange.com/questions/31579/what-is-the-optimal-k-for-the-k-nearest-neighbour-classifier-on-the-iris-dat
# https://gist.github.com/zachmayer/3061272
#Multi-Class Summary Function
#Based on caret:::twoClassSummary
require(compiler)
multiClassSummary <- cmpfun(function (data, lev = NULL, model = NULL)
{
#Load Libraries
require(Metrics)
require(caret)
#Check data
if (!all(levels(data[, "pred"]) == levels(data[, "obs"])))
stop("levels of observed and predicted data do not match")
#Calculate custom one-vs-all stats for each class
prob_stats <- lapply(levels(data[, "pred"]), function(class)
{
#Grab one-vs-all data for the class
pred <- ifelse(data[, "pred"] == class, 1, 0)
obs <- ifelse(data[, "obs"] == class, 1, 0)
prob <- data[,class]
#Calculate one-vs-all AUC and logLoss and return
cap_prob <- pmin(pmax(prob, .000001), .999999)
prob_stats <- c(auc(obs, prob), logLoss(obs, cap_prob))
names(prob_stats) <- c("ROC", "logLoss")
return(prob_stats)
})
prob_stats <- do.call(rbind, prob_stats)
rownames(prob_stats) <- paste( "Class:" , levels(data[, "pred"]))
#Calculate confusion matrix-based statistics
CM <- confusionMatrix(data[, "pred"], data[, "obs"])
#Aggregate and average class-wise stats
#Todo: add weights
class_stats <- cbind(CM$byClass, prob_stats)
class_stats <- colMeans(class_stats)
#Aggregate overall stats
overall_stats <- c(CM$overall)
#Combine overall with class-wise stats and remove some stats we don't want
stats <- c(overall_stats, class_stats)
stats <- stats[! names(stats) %in% c("AccuracyNull","Prevalence", "Detection Prevalence")]
#Clean names and return
names(stats) <- gsub('[[:blank:]] +', '_' , names(stats))
return(stats)
})
## Note: no visible binding for global variable 'Metrics'
## Note: no visible binding for global variable 'caret'
## set up working directory
setwd(paste (root, "/myGit/mixturemodel/reconData/para1/", sep=""))
## read in data from txt file
data <- read.table("recon_3classes_para1.txt", header=TRUE, sep = "\t")
##### BEGIN: data partition >>>>>
## set random seed
set.seed(12345)
#set.seed(34546)
## create data partition
inTrainingSet <- createDataPartition(data$label, p=.7, list=FALSE)
labelTrain <- data[ inTrainingSet,]
labelTest <- data[-inTrainingSet,]
nrow(labelTrain)
nrow(labelTest)
##### END: data partition <<<<<
##### BEGIN: tune the parameters >>>>>
## control:
# resampling technique: 5-repeat 10-fold cross-validation
# performance metrics: ROC AUC curve
ctrl <- trainControl(method = "repeatedcv",
repeats = 5,
summaryFunction = multiClassSummary,
classProbs = TRUE)
##### END: tune the parameters <<<<<
##### BEGIN: train model - svm >>>>>
set.seed(1024)
svmFit <- train(label ~ ., data = labelTrain,
## training model: svm >>>
method = "svmRadial",
metric = "ROC",
tuneLength = 10,
trControl = ctrl)
## prediction
svmPred <- predict(svmFit, labelTest)
str(svmPred)
## predicted probabilities
svmProbs <- predict(svmFit, labelTest, type = "prob")
str(svmProbs)
confusionMatrix(svmPred, labelTest$label)
setwd(paste (root, "/myGit/mixturemodel/reconData/para2/", sep=""))
data <- read.table("recon_3classes_para4.txt", header=TRUE, sep = "\t")
##### BEGIN: data partition >>>>>
## set random seed
set.seed(12345)
#set.seed(34546)
## create data partition
inTrainingSet <- createDataPartition(data$label, p=.7, list=FALSE)
labelTrain <- data[ inTrainingSet,]
labelTest <- data[-inTrainingSet,]
nrow(labelTrain)
nrow(labelTest)
##### END: data partition <<<<<
##### BEGIN: tune the parameters >>>>>
## control:
# resampling technique: 5-repeat 10-fold cross-validation
# performance metrics: ROC AUC curve
ctrl <- trainControl(method = "repeatedcv",
repeats = 5,
summaryFunction = multiClassSummary,
classProbs = TRUE)
##### END: tune the parameters <<<<<
##### BEGIN: train model - svm >>>>>
set.seed(1024)
svmFit <- train(label ~ ., data = labelTrain,
## training model: svm >>>
method = "svmRadial",
metric = "ROC",
tuneLength = 10,
trControl = ctrl)
## prediction
svmPred <- predict(svmFit, labelTest)
str(svmPred)
## predicted probabilities
svmProbs <- predict(svmFit, labelTest, type = "prob")
str(svmProbs)
confusionMatrix(svmPred, labelTest$label)
